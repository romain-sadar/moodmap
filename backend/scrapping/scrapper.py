import pandas as pd
import time
from selenium import webdriver
from selenium.webdriver.common.by import By

# üìå Configuration de Selenium
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # Ex√©cution en arri√®re-plan (optionnel)
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

# üî• Lance le WebDriver Chrome
driver = webdriver.Chrome(options=options)

# üìÇ Charger le fichier CSV contenant les lieux et leurs URLs
file_path = "/Users/erwanhamza/moodmap/backend/places_singapore.csv"  # Mets ici le bon chemin vers ton fichier CSV
df_places = pd.read_csv(file_path)

# üìå V√©rification des colonnes attendues
if "label" not in df_places.columns or "link" not in df_places.columns:
    raise ValueError("Le fichier CSV doit contenir les colonnes 'label' et 'link'.")

# üè¢ Liste des lieux avec leurs URLs
places_urls = df_places[["label", "link"]].dropna().to_dict(orient="records")


# üîç Fonction pour scraper les avis
def get_google_reviews(place):
    driver.get(place["link"])
    time.sleep(5)  # Laisse le temps de charger la page

    try:
        # ‚úÖ Trouver et cliquer sur le bouton "Avis"
        reviews_button = driver.find_element(
            By.XPATH, '//button[contains(@aria-label, "avis")]'
        )
        reviews_button.click()
        time.sleep(3)

        # ‚úÖ Scroller pour charger plus d'avis
        scrollable_div = driver.find_element(
            By.XPATH, '//div[contains(@class, "m6QErb")]'
        )
        for _ in range(5):  # Ajuster le nombre de scrolls si besoin
            driver.execute_script(
                "arguments[0].scrollTop = arguments[0].scrollHeight", scrollable_div
            )
            time.sleep(2)

        # ‚úÖ Extraction des avis
        reviews = []
        review_elements = driver.find_elements(
            By.XPATH, '//div[contains(@class, "jftiEf fontBodyMedium")]'
        )

        for review in review_elements[:10]:  # R√©cup√©rer entre 5 et 10 avis
            try:
                author = review.find_element(
                    By.XPATH, './/div[contains(@class, "d4r55")]'
                ).text
                rating = review.find_element(
                    By.XPATH, './/span[contains(@class, "kvMYJc")]'
                ).get_attribute("aria-label")
                review_text = review.find_element(
                    By.XPATH, './/span[contains(@class, "wiI7pd")]'
                ).text
                reviews.append(
                    {
                        "place": place["label"],
                        "author": author,
                        "rating": rating,
                        "review": review_text,
                    }
                )
            except:  # noqa E722
                continue

        return reviews
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du scraping de {place['label']} : {e}")
        return None


# üìä Stocker les r√©sultats
all_reviews = []
for place in places_urls:
    print(f"üìç Scraping : {place['label']}")
    reviews = get_google_reviews(place)
    if reviews:
        all_reviews.extend(reviews)
    time.sleep(2)  # Pause pour √©viter le blocage

# üìù Convertir en DataFrame et sauvegarder en CSV
df_reviews = pd.DataFrame(all_reviews)
df_reviews.to_csv("google_reviews.csv", index=False, encoding="utf-8")

print("\n‚úÖ Avis sauvegard√©s avec succ√®s dans 'google_reviews.csv' !")

# üî¥ Fermeture du navigateur Selenium
driver.quit()
